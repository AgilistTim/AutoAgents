import os
import json
import asyncio
import argparse
from tqdm.asyncio import tqdm_asyncio
from langchain.schema import HumanMessage
from langchain.chat_models import ChatOpenAI


PARENT_DIR: str = os.path.dirname(os.path.abspath(__file__))
DATASET_FILE: str = os.path.join(PARENT_DIR, "transformed.json")
NUM_DATA: int = 100
EVAL_MODEL_NAME: str = "gpt-4"
AWAIT_TIMEOUT: int = 360
RETRY_COUNT = 2


def get_dataset(dataset_file: str = DATASET_FILE):
    with open(dataset_file, 'r') as f:
        dataset = json.load(f)
    return dataset


async def evaluate(data, model, results):

    conversations = data["conversations"]
    history = conversations[0]["value"]
    model_output = conversations[1]["value"]

    prompt = f"""Given an input with a final goal, a set of detailed instructions and a history of the query/thoughts/plan/action/observation, the search agent need to generate a next step plan to fulfill user's input query. Evalluate a set of plans generated by a search agent. The input and the generated plans are both delimited by triple backticks.

Input:
```
{history}
```

Next step plan of Agent:
```
{model_output}
```

Consider evaluate the next step plan based on the following aspects/metrics:
- Clarity: The plan should be clear and understandable.
- Effectiveness: The plan should be correct and move towards the final goal to answer the input query.

Your answer should include a score on a scale of 0 to 5, where 0 means terrible, 1 means bad, 2 means acceptable, 3 means okay, 4 means good, and 5 means wonderful. Format your output in a json consisting of overall_score, overall_judgement, clarity_score, clarity_reasoning, effectiveness_score, and effectiveness_reasoning.
    """

    llm = ChatOpenAI(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        openai_organization=os.getenv("OPENAI_API_ORG"),
        temperature=0,
        model=model,
        request_timeout=AWAIT_TIMEOUT
    )

    retry_cnt = 0
    while retry_cnt <= RETRY_COUNT:
        try:
            resp = await llm.agenerate([[HumanMessage(content=prompt)]])
            resp_obj = json.loads(resp.generations[0][0].text.strip())
            resp_obj["data_id"] = data["id"]

            results.append(resp_obj)
            break

        except Exception as e:
            print(e)
            retry_cnt += 1


async def main(dataset, model, num_data):

    results = []

    semaphore = asyncio.Semaphore(10)

    async def process_data(data):
        async with semaphore:
            await evaluate(data, model, results)

    await tqdm_asyncio.gather(*[process_data(data) for data in dataset])

    with open(f"response_eval_{model}_{num_data}.json", 'w') as f:
        json.dump(results, f, indent=2)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--dataset_file", type=str, default=DATASET_FILE,
        help="file containing the dataset"
    )
    parser.add_argument("--eval_model", type=str, default=EVAL_MODEL_NAME)
    parser.add_argument("--num_data", type=int, default=NUM_DATA)
    args = parser.parse_args()

    dataset = get_dataset(dataset_file=args.dataset_file)[:args.num_data]
    asyncio.run(main(dataset, args.eval_model, args.num_data))
