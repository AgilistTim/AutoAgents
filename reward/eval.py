import os
import json
import asyncio
from tqdm.asyncio import tqdm_asyncio
from langchain.schema import HumanMessage
from langchain.chat_models import ChatOpenAI


PARENT_DIR: str = os.path.dirname(os.path.abspath(__file__))
DATASET_FILE: str = os.path.join(PARENT_DIR, "transformed.json")
NUM_DATA: int = 100
EVAL_MODEL_NAME: str = "gpt-3.5-turbo"
AWAIT_TIMEOUT: int = 360


def get_dataset():
    with open(DATASET_FILE, 'r') as f:
        dataset = json.load(f)
    return dataset


async def evaluate(data, results):

    conversations = data["conversations"]
    history = conversations[0]["value"]
    model_output = json.loads(conversations[1]["value"])

    prompt = f"""Given an input with a final goal, a set of detailed instructions and a history of the query/thoughts/plan/action/observation, the search agent need to generate a next step plan to fulfill user's input query. Evalluate a set of plans generated by a search agent. The input and the generated plans are both delimited by triple backticks.

Input:
```
{history}
```

Next step plan of Agent:
```
{model_output}
```

Consider evaluate the next step plan based on the following aspects/metrics:
- Clarity: The plan should be clear and understandable.
- Effectiveness: The plan should be correct and move towards the final goal to answer the input query.

Your answer should include a score on a scale of 0 to 5, where 0 means terrible, 1 means bad, 2 means acceptable, 3 means okay, 4 means good, and 5 means wonderful. Format your output in a json consisting of overall_score, overall_judgement, clarity_score, clarity_reasoning, effectiveness_score, and effectiveness_reasoning.
    """

    llm = ChatOpenAI(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        openai_organization=os.getenv("OPENAI_API_ORG"),
        temperature=0,
        model=EVAL_MODEL_NAME,
        request_timeout=AWAIT_TIMEOUT
    )

    try:
        resp = await llm.agenerate([[HumanMessage(content=prompt)]])
        resp_obj = json.loads(resp.generations[0][0].text.strip())
        resp_obj["history"] = history
        resp_obj["response"] = model_output

        results.append(resp_obj)

    except Exception as e:
        print(e)


async def main():

    results = []

    dataset = get_dataset()[:NUM_DATA]

    semaphore = asyncio.Semaphore(10)

    async def process_data(data):
        async with semaphore:
            await evaluate(data, results)

    await tqdm_asyncio.gather(*[process_data(data) for data in dataset])

    with open(f"response_eval_{EVAL_MODEL_NAME}_{NUM_DATA}.json", 'w') as f:
        json.dump(results, f, indent=2)


if __name__ == "__main__":
    asyncio.run(main())
